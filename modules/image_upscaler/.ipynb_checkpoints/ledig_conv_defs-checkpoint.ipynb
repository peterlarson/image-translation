{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-1-ecae0553c188>, line 57)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-1-ecae0553c188>\"\u001b[1;36m, line \u001b[1;32m57\u001b[0m\n\u001b[1;33m    bias_variable([f_out],stddev=stddev_factor)\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "def show_image(image):\n",
    "        plt.matshow(image, cmap=plt.cm.gray)\n",
    "        \n",
    "def minibatch_image_generator(image_filenames, batch_size):\n",
    "    epoch = 0 \n",
    "    while 1:\n",
    "        scrambled_images = np.random.permutation([misc.imread(img) for img in image_filenames])\n",
    "        for batch_number in range(int(np.ceil(len(scrambled_images)/batch_size))):\n",
    "            start_index = batch_number*batch_size\n",
    "            end_index = min(len(scrambled_images),start_index+batch_size)\n",
    "            yield epoch, scrambled_images[start_index:end_index]\n",
    "        epoch = epoch + 1\n",
    "        \n",
    "def weight_variable(shape, stddev=0.1):\n",
    "  initial = tf.truncated_normal(shape, stddev=stddev)\n",
    "  return tf.Variable(initial)\n",
    "\n",
    "def _glorot_initializer(prev_units, num_units, stddev_factor=1.0):\n",
    "    \"\"\"Initialization in the style of Glorot 2010.\n",
    "\n",
    "    stddev_factor should be 1.0 for linear activations, and 2.0 for ReLUs\"\"\"\n",
    "    stddev  = np.sqrt(stddev_factor / np.sqrt(prev_units*num_units))\n",
    "    return tf.truncated_normal([prev_units, num_units],\n",
    "                                mean=0.0, stddev=stddev)\n",
    "\n",
    "def _glorot_initializer_conv2d(prev_units, num_units, mapsize, stddev_factor=1.0):\n",
    "    \"\"\"Initialization in the style of Glorot 2010.\n",
    "\n",
    "    stddev_factor should be 1.0 for linear activations, and 2.0 for ReLUs\"\"\"\n",
    "\n",
    "    stddev  = np.sqrt(stddev_factor / (np.sqrt(prev_units*num_units)*mapsize*mapsize))\n",
    "    return tf.truncated_normal([mapsize, mapsize, prev_units, num_units],\n",
    "                                mean=0.0, stddev=stddev)\n",
    "\n",
    "def bias_variable(shape, stddev=0.1):\n",
    "    #TODO: stddev arg has no effect\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def conv2d(x, W, stride_input = [1, 2, 2, 1]):\n",
    "    return tf.nn.conv2d(x, W, strides = stride_input, padding='SAME')\n",
    "\n",
    "def relu(input, leak = 0.2):\n",
    "    #TODO: implement leaky relu\n",
    "\n",
    "    return tf.nn.relu(input)\n",
    "\n",
    "def full_conv2d(input_tensor, f_in, f_out, mapsize=3, stride=1, stddev_factor=0.1, weights=None):\n",
    "    if weights == None:\n",
    "        (weight,bias) = conv2d_wb_initiator(f_in, f_out, mapsize=mapsize)\n",
    "    else: \n",
    "        (weight,bias) = weights\n",
    "    return conv2d(input_tensor, weight, stride_input = [1, stride, stride, 1]) + bias\n",
    "\n",
    "def conv2d_wb_initiator(f_in, f_out, mapsize=3, stddev_factor=0.1):\n",
    "    return weight_variable([mapsize, mapsize, f_in, f_out], stddev=stddev_factor),\\\n",
    "           bias_variable([f_out],stddev=stddev_factor)\n",
    "\n",
    "def batch_norm(input_tensor, scale=False):\n",
    "    return tf.contrib.layers.batch_norm(input_tensor, scale=scale)\n",
    "\n",
    "def add_residual_block(input_tensor, f_in, f_internal, mapsize=3, num_layers=2,\\\n",
    "                       stddev_factor=1e-3, relu_pre_add = False):\n",
    "    \"\"\"Adds a residual block as per Arxiv 1512.03385, Figure 3\"\"\"\n",
    "\n",
    "    current = input_tensor\n",
    "    \n",
    "    #Bring size of tensor to correct size\n",
    "    if(f_in != f_internal):\n",
    "        current = full_conv2d(current, f_in, f_internal, mapsize=1, stddev_factor=1.)\n",
    "    \n",
    "    bypass = current\n",
    "    \n",
    "    # Residual block\n",
    "    for _ in range(num_layers-1):\n",
    "        current = full_conv2d(current, f_internal, f_internal, stddev_factor=stddev_factor)\n",
    "        current = batch_norm(current)\n",
    "        current = relu(current)\n",
    "    current = full_conv2d(current, f_internal, f_internal, stddev_factor=stddev_factor)\n",
    "    current = batch_norm(current)\n",
    "    if(relu_pre_add):\n",
    "        current = relu(current)\n",
    "        \n",
    "    current = tf.add(current,bypass)\n",
    "\n",
    "    return current\n",
    "\n",
    "def upscale(in_tensor, size):\n",
    "    return tf.image.resize_bilinear(in_tensor, size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
